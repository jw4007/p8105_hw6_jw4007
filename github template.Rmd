---
title: "hw6_jw4007"
output: github_document
---


```{r}
library(tidyverse)
library(modelr)
library(mgcv)
library(p8105.datasets)

set.seed(1)
```

## Q2
### Importing data
```{r}
homicide_url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicide = read.csv(homicide_url) %>% janitor::clean_names()

```
There is `r nrow(homicide)` observations and `r ncol(homicide)` variables in the dataset. The variables are `r names(homicide)`

### Prepare data for analysis.
```{r}
homicide_analyze = homicide %>% 
  mutate(case_solve = if_else(disposition == "Closed by arrest", 1, 0),
         city_state = str_c(city, state, sep = ","),
         victim_age = as.numeric(victim_age)) %>%
  filter(city_state != "Dallas,TX",
         city_state != "Phoenix,AZ", 
         city_state != "Kansas City,MO", 
         city_state != "Tulsa,AL") %>% 
  filter(victim_race == "White" | victim_race == "Black")
  
```
Create a city_state variable and a binary variable indicating whether the homicide is solved. Omit cities, limit "victim_race" is white or black. Mutate "victim_age" as a numeric variable. There is `r nrow(homicide_analyze)` observations and `r ncol(homicide_analyze)` variables in the dataset. The variables are `r names(homicide_analyze)`

### Fit a logistic regression for Baltimore.
```{r}
Baltimore_glm = lm(case_solve ~ victim_age + victim_sex + victim_race, data = homicide_analyze) %>% 
  broom::tidy() 

Baltimore_glm %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(lower_bound = exp(estimate - 1.96*std.error),
         upper_bound = exp(estimate + 1.96*std.error),
         odds_ratio = exp(estimate)) %>% 
  select(estimate, odds_ratio, lower_bound, upper_bound) %>% knitr::kable(digits = 2)
```
Outcome = resolved vs unresolved;
Predictors =  victim age, sex and race.
The above table shows the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.


### Fit a logistic regression for all the cities in the dataset.
```{r}
homicide_male_vs_female = homicide_analyze %>% 
  nest(data = !city_state) %>% 
  mutate(models = map(.x = data, ~lm(case_solve ~ victim_age + victim_sex + victim_race, data = .x)),
         results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(lower_bound = exp(estimate - 1.96*std.error),
         upper_bound = exp(estimate + 1.96*std.error),
         OR_male_vs_female = exp(estimate)) %>% 
  select(city_state, estimate, OR_male_vs_female, lower_bound, upper_bound) 

knitr::kable(homicide_male_vs_female, digits = 2)
  
```
The above table shows the adjusted odds ratios (and CIs) for solving homicides comparing male victims to female victims in all cities keeping all other variables fixed.


### Plot the ORs and CIs for each city.
```{r}
homicide_male_vs_female %>% 
  mutate(city_state = fct_reorder(city_state, OR_male_vs_female, .desc = TRUE)) %>% 
  ggplot(aes(x = OR_male_vs_female, y = city_state)) + 
  geom_point() +
  geom_errorbar(aes(xmin = lower_bound, xmax = upper_bound))
```
The above plot shows that New York City has the lowest odds for solving homicides when comparing male victims to female victims. The odds for solving homicides for male victims is more than 20% lower than female victims, keeping all other variables fixed. It is statistically significant at a level of 5%. On the other hand, the odds for solving homicides when comparing male victims to female victims Albuquerque is the highest, keeping all other variables fixed. The odds for solving homicides for male victims is 1.1 times the odds for solving homicides for female victims, keeping all other variables fixed. However, this odds is not statistically significant.


## Q3
### Import data
```{r}
birthweight = read.csv("./birthweight.csv") %>% 
  janitor::clean_names() 

birthweight %>% 
  summarize(n = sum(is.na(birthweight)))
```

### data preparation
```{r}
birthweight = birthweight %>% 
  mutate(babysex = as.factor(babysex),
         delwt_kg = delwt * 0.4536,
         malform = as.factor(malform),
         frace = as.factor(frace),
         mrace = as.factor(mrace),
         mheight_cm = mheight * 2.54,
         ppwt_kg = ppwt * 0.4536,
         wtgain_kg = wtgain * 0.4536) 
```

Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process and show a plot of model residuals against fitted values â€“ use add_predictions and add_residuals in making this plot.

### Fit the hypothesized regression modle for birthweight.
```{r}
birthweight_hyp = lm(bwt ~ blength + babysex + malform, data = birthweight) 

birthweight %>% 
  add_predictions(birthweight_hyp) %>% 
  add_residuals(birthweight_hyp) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point() 
```
I hypothesized the birthweight is associated with the baby's length at birth, baby's sex, and the presence of malformations that could affect weight.
The above plot shows that the residuals spread somewhat equally around 0, with few outliers when prediction of birthweight is above 4000 and below 1000.


### Fit the main model and the interaction model and cross validate caculating root-mean-squared-error on testing data.
```{r}
birthweight_cv = 
  crossv_mc(birthweight, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>% 
  mutate(hyp = map(train, ~lm(bwt ~ blength + babysex + malform, data = .x)),
         main = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         interaction = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_hyp = map2_dbl(hyp, test, ~rmse(model = .x, data = .y)),
         rmse_main = map2_dbl(main, test, ~rmse(model = .x, data = .y)),
         rmse_interaction = map2_dbl(interaction, test, ~rmse(model = .x, data = .y)))

```
Predictors for main effect model: length at birth and gestational age;
Predictors for interaction model: head circumference, length, sex, and all interactions (including the three-way interaction) between these.


### Plot to compare models
```{r}
birthweight_cv %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(everything(),
               names_to = "model",
               values_to = "rmse",
               names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_boxplot()

```
The above boxplot shows that the interaction model has the lowest root-mean-squared-error and the hypothesized model has the biggest. So, the interaction model has the best fit to the data among the three models.
